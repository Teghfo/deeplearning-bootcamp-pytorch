{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dXg7GYtM0PD"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "K_EpVSnLTLr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Hyperparameter\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "image_size = 72\n",
        "patch_size = 6\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "batch_size = 32\n",
        "hidden_size = 64"
      ],
      "metadata": {
        "id": "62ZwnBaDPT9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "\n",
        "train_ds = datasets.CIFAR100(root=\"./data\", download=True, train=True, transform=transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "qoVFWATqN3Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"data batch size: {}\".format(next(iter(train_dl))[0].size()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puLHLXGFPvQJ",
        "outputId": "3672e49a-4af9-4e7d-8692-d4d8bc6020a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch size: torch.Size([32, 3, 72, 72])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchLayerWithoutChannel(nn.Module):\n",
        "    def __init__(self, patch_size: int):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    # input = (N, C, H, W) ===> return = (N, CHW/ P^2, P^2)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.size(2) == x.size(3), \"image size should be square\"\n",
        "        HW = x.size(2) ** 2\n",
        "        channel_in = x.size(1)\n",
        "        assert HW % (self.patch_size ** 2) == 0, \"image size should be devisible by patch!\"\n",
        "        patch_num = HW // (self.patch_size ** 2)\n",
        "        patch = torch.zeros(x.size(0), patch_num, channel_in, self.patch_size , self.patch_size)\n",
        "        for i in range(x.size(2) // self.patch_size):\n",
        "            for j in  range(x.size(3) // self.patch_size):\n",
        "                    patch[:, i * (x.size(2) // self.patch_size) + j, :, :] = x[..., i * self.patch_size:(i+1) * self.patch_size, j * self.patch_size:(j+1) * self.patch_size]\n",
        "        return patch\n",
        "img = train_ds[1][0]\n",
        "plt.imshow(img.detach().cpu().numpy().transpose(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "patcher = PatchLayerWithoutChannel(6)\n",
        "patched_img = patcher(img.unsqueeze(0))\n",
        "patched_img[0, 0].size()\n",
        "train_ds[5][0].detach().cpu().numpy()\n",
        "fig, axes = plt.subplots(12, 12, figsize=(6, 6))\n",
        "for i in range(12):\n",
        "    for j in range(12):\n",
        "        axes[i, j].imshow(patched_img[0, i * 12 + j].detach().cpu().numpy().transpose(1, 2, 0))\n",
        "        axes[i, j].axis(\"off\")"
      ],
      "metadata": {
        "id": "jXUS_7gO8zKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## step 1: patchify Layer\n",
        "class PatchLayer(nn.Module):\n",
        "    def __init__(self, patch_size: int, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.device = device\n",
        "    # input = (N, C, H, W) ===> return = (N, CHW/ P^2, P^2)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.size(2) == x.size(3), \"image size should be square\"\n",
        "        HW = x.size(2) ** 2\n",
        "        channel_in = x.size(1)\n",
        "        assert HW % (self.patch_size ** 2) == 0, \"image size should be devisible by patch!\"\n",
        "        patch_num = (channel_in * HW) // (self.patch_size ** 2)\n",
        "        patch = torch.zeros(x.size(0), patch_num, self.patch_size ** 2, device=device)\n",
        "        for i in range(x.size(2) // self.patch_size):\n",
        "            for j in  range(x.size(3) // self.patch_size):\n",
        "                for c in range(channel_in):\n",
        "                    temp = x[:, c, i * self.patch_size:(i+1) * self.patch_size, j * self.patch_size:(j+1) * self.patch_size]\n",
        "                    patch[:, c + (2 * i) + 1, :] = temp.reshape(x.size(0), -1)\n",
        "        return patch\n",
        "\n",
        "patcher = PatchLayer(6)\n",
        "images, _ = next(iter(train_dl))\n",
        "patcher(images).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwqB4CMMNw62",
        "outputId": "c18e8a61-4cb2-4145-a13d-31350959c8fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 432, 36])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## step 2: Map Layer\n",
        "class LinearMap(nn.Module):\n",
        "    def __init__(self, patch_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(patch_size ** 2, hidden_size)\n",
        "\n",
        "    # input = (N, S, d) ===> return = (N, S, d')\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.fc(x)\n",
        "\n",
        "patcher = PatchLayer(6)\n",
        "mapper = LinearMap(6, 8)\n",
        "images, _ = next(iter(train_dl))\n",
        "mapper(patcher(images)).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNQ4HKZtQg9x",
        "outputId": "f324caa2-ccbc-4be8-af7e-30b20db161af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 432, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## step 3: Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, num_patches, hidden_size) -> None:\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        pe = torch.zeros(1, num_patches + 1, hidden_size)\n",
        "        position = torch.arange(0, num_patches + 1).float().unsqueeze(1)\n",
        "        step_compnent = torch.arange(0, hidden_size, 2).float()\n",
        "        w = torch.exp(step_compnent * (-math.log(10_000) / hidden_size))\n",
        "\n",
        "        pe[0, :, 0::2] = torch.sin(w * position)\n",
        "        pe[0, :, 1::2] = torch.cos(w * position)\n",
        "\n",
        "        self.v_class = nn.Parameter(torch.randn(1, hidden_size))\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([self.v_class.expand(x.size(0), 1, -1), x], dim=1)\n",
        "        return x + self.pe\n",
        "\n",
        "\n",
        "patcher = PatchLayer(patch_size)\n",
        "mapper = LinearMap(patch_size, hidden_size)\n",
        "po_en = PositionalEncoding(num_patches * 3, hidden_size)\n",
        "images, _ = next(iter(train_dl))\n",
        "po_en(mapper(patcher(images))).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg8oTAhCQ4tE",
        "outputId": "4a39d416-9fbf-4748-e157-e25939374a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 433, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## step 4: Transformer Block\n",
        "class MultiHead(nn.Module):\n",
        "    def __init__(self, d_model=512, num_head=8, mask=None,  *args, **kwargs) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "        assert d_model % num_head == 0, \"d_model divisible by num_head\"\n",
        "        self.d_model = d_model\n",
        "        self.num_head = num_head\n",
        "        self.mask = mask\n",
        "        self.dim_h = d_model // num_head\n",
        "\n",
        "        self.linear_k = nn.Linear(d_model, d_model)\n",
        "        self.linear_q = nn.Linear(d_model, d_model)\n",
        "        self.linear_v = nn.Linear(d_model, d_model)\n",
        "        self.linear_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_(self, x):\n",
        "        return x.view(x.size(0), x.size(1) * self.num_head, self.dim_h)\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
        "        btz, seq_len, _ = query.size()\n",
        "        # Linear + split\n",
        "        q, k, v = self.split_(self.linear_q(query)), self.split_(self.linear_k(\n",
        "            key)), self.split_(self.linear_v(value))\n",
        "\n",
        "        _, _, d_key = k.size()\n",
        "        query_scaled = q / math.sqrt(d_key)\n",
        "\n",
        "        attn_output_weights = torch.bmm(query_scaled, k.transpose(-2, -1))\n",
        "\n",
        "        if self.mask:\n",
        "            attn_output_weights.masked_fill_(self.mask == 0, -1e12)\n",
        "\n",
        "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
        "\n",
        "        atten_output = torch.bmm(attn_output_weights, v)\n",
        "\n",
        "        # concat\n",
        "        atten_output = atten_output.transpose(\n",
        "            0, 1).contiguous().view(btz * seq_len, self.d_model)\n",
        "        atten_output = self.linear_out(atten_output)\n",
        "        return atten_output.view(btz, seq_len, self.d_model)\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model=64, num_head=8, mask=None):\n",
        "        super().__init__()\n",
        "        self.nl1 = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHead(d_model=d_model, num_head=num_head)\n",
        "        self.nl2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.nl1(x), self.nl1(x), self.nl1(x))\n",
        "        x = x + self.mlp(self.nl2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self,  d_model=64, num_head=8, mask=None, L: int=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.ModuleList([])\n",
        "        for i in range(L):\n",
        "            self.encoder.append(TransformerEncoderBlock(d_model=d_model, num_head=num_head, mask=mask))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.encoder:\n",
        "            x = module(x)\n",
        "        return x\n",
        "\n",
        "x = torch.randn(2, 433, 64)\n",
        "tb = TransformerEncoder(d_model=hidden_size, num_head=8)\n",
        "tb(x).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zShdYCoREeC",
        "outputId": "bbf99d6c-a815-4218-f8c2-536a805e614c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 433, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## step 5: classifier Block\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, dropout=None, model_dim=64, target_size=100):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.fc = nn.Linear(model_dim, target_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.dropout:\n",
        "            x = F.dropout(x, self.dropout)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "ZKAjM3FIRMY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VIT(nn.Module):\n",
        "    def __init__(self, device=\"cuda\", patch_size: int = 6, hidden_size: int = 64, target_size: int=100, num_patches: int = 432, num_head=8, mask=None, L: int=2):\n",
        "        super().__init__()\n",
        "        self.patcher = PatchLayer(patch_size, device=device)\n",
        "        self.mapper = LinearMap(patch_size, hidden_size)\n",
        "        self.pe = PositionalEncoding(num_patches, hidden_size)\n",
        "        self.transformer_encoder = TransformerEncoder(d_model=hidden_size, num_head=num_head, mask=mask, L=L)\n",
        "        self.classifier = Classifier(dropout=0.5, model_dim=hidden_size, target_size=target_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patcher(x)\n",
        "        x = self.mapper(x)\n",
        "        x = self.pe(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.classifier(x[:, 0, :])\n",
        "        return x\n",
        "\n",
        "x = torch.randn(2, 3, 72, 72).to(device)\n",
        "vit = VIT().to(device)\n",
        "vit(x).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUXo3VDeRPNY",
        "outputId": "6f304eb1-1cbd-4e46-a70a-cc95837550b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VIT(device=device).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "2CjyanVgR-kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    for images, labels in tqdm(train_dl):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = model(images)\n",
        "        loss = criterion(prediction, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} | train loss: {train_loss/len(train_dl)}')"
      ],
      "metadata": {
        "id": "xwVvrvVWRcOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}