{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcTc6WQfQyq1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Progressive growing of layers\n",
        "# Minibatch std layer (Discriminator)\n",
        "# Equalized learning rate\n",
        "# Pixelwise normalization layer (Generator)"
      ],
      "metadata": {
        "id": "_IR_f6HWRej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs / torch.sqrt(torch.mean(torch.square(X), axis=1, keepdim=True) + 1e-8)\n",
        "\n",
        "class PixelWiseNormalization(nn.Module):\n",
        "    def __init__(self, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return (x / torch.sqrt(torch.mean(torch.square(x), dim=1, keepdim=True)\n",
        "         + self.eps))\n",
        "\n",
        "# x = torch.randn(32, 3, 32, 32)\n",
        "# model = PixelWiseNormalization()\n",
        "# model(x).size()"
      ],
      "metadata": {
        "id": "eMtCCaYuU5wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualizedConv2d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=0, gain=2):\n",
        "        super().__init__()\n",
        "        self.conv2d: nn.Conv2d = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding)\n",
        "        self.fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.conv2d.weight)\n",
        "        self.wscale = torch.sqrt(gain / torch.tensor(self.fan_in))\n",
        "        self.reset_weights()\n",
        "\n",
        "    def reset_weights(self):\n",
        "        nn.init.normal_(self.conv2d.weight) * self.wscale\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv2d(x)\n",
        "\n",
        "\n",
        "# x = torch.randn(2, 3, 5, 5)\n",
        "# model = EqualizedConv2d(3, 6, 3)\n",
        "# model(x).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_WZFfmfXV9D",
        "outputId": "3465c6c6-3f06-4bef-f2fc-2a8d4d9a7e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualizedConvTranspose2d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=0, gain=2):\n",
        "        super().__init__()\n",
        "        self.conv_transpose2d: nn.ConvTranspose2d = nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride, padding)\n",
        "        self.fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.conv_transpose2d.weight)\n",
        "        self.wscale = torch.sqrt(gain / torch.tensor(self.fan_in))\n",
        "        self.reset_weights()\n",
        "\n",
        "    def reset_weights(self):\n",
        "        nn.init.normal_(self.conv_transpose2d.weight) * self.wscale\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_transpose2d(x)\n",
        "\n",
        "\n",
        "# x = torch.randn(2, 512, 1, 1)\n",
        "# model = EqualizedConvTranspose2d(512, 512, 4)\n",
        "# model(x).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLQB4-m7sMEF",
        "outputId": "e9545ac3-f634-405f-aedb-8783957f3d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 512, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# S = tf.math.reduce_std(inputs, axis=[0, -1])\n",
        "# v = tf.reduce_mean(S)\n",
        "# tf.concat([inputs, tf.fill([batch_size, height, width, 1], v)], axis=-1)\n",
        "\n",
        "class MiniBatchSTD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        S = torch.std(x, dim=(0, 1))\n",
        "        v = torch.mean(S)\n",
        "        statistics = torch.empty(x.size(0), 1, x.size(2), x.size(3))\n",
        "        statistics.fill_(v)\n",
        "\n",
        "        return torch.cat([x, statistics], axis=1)\n",
        "\n",
        "# x = torch.randn(2, 3, 5, 5)\n",
        "# model = MiniBatchSTD()\n",
        "# model(x)[:, 3, ...]"
      ],
      "metadata": {
        "id": "vH2pVf46Z_aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BlockConv2d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, leakyscale=0.2, use_pn=True):\n",
        "        super().__init__()\n",
        "        self.use_pn = use_pn\n",
        "        self.cn1 = EqualizedConv2d(in_ch, out_ch, kernel_size, stride=stride, padding=padding)\n",
        "        self.leaky_relu1 = nn.LeakyReLU(leakyscale)\n",
        "        self.cn2 = EqualizedConv2d(out_ch, out_ch, kernel_size, stride=stride, padding=padding)\n",
        "        self.leaky_relu2 = nn.LeakyReLU(leakyscale)\n",
        "        self.pn = PixelWiseNormalization()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu1(self.cn1(x))\n",
        "        if self.use_pn:\n",
        "            x = self.pn(x)\n",
        "        x = self.leaky_relu2(self.cn2(x))\n",
        "        if self.use_pn:\n",
        "            x = self.pn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "x = torch.randn(2, 3, 5, 5)\n",
        "model = BlockConv2d(3, 10)\n",
        "model(x).size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7vhU8mvlIZG",
        "outputId": "5b870d28-badc-4d22-9c58-436f15924698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n",
        "\n",
        "F.upsample(input, scale_factor=2)\n",
        "m(input).size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAWt5fl23-LA",
        "outputId": "eda34ce3-910b-4b7c-8e82-19aecb35ca83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent vector ---> 512 × 1 × 1\n",
        "############# step0 ===> initial block (4 * 4)\n",
        "# TransposeConv2d 4 × 4 LReLU ---> 512 × 4 × 4\n",
        "# Conv 3 × 3 LReLU ----> 512 × 4 × 4\n",
        "############ step1 (8 * 8)\n",
        "# Upsample ->  512 × 8 × 8\n",
        "# Conv 3 × 3 LReLU --> 512 × 8 × 8\n",
        "# Conv 3 × 3 LReLU --> 512 × 8 × 8\n",
        "############ step2 (16 * 16)\n",
        "# Upsample –> 512 × 16 × 16\n",
        "# Conv 3 × 3 LReLU 512 × 16 × 16\n",
        "# Conv 3 × 3 LReLU 512 × 16 × 16\n",
        "############ step3 (32 * 32)\n",
        "# Upsample –> 512 × 32 × 32\n",
        "# Conv 3 × 3 LReLU 512 × 32 × 32\n",
        "# Conv 3 × 3 LReLU 512 × 32 × 32\n",
        "############ step4 (64 * 64)\n",
        "# Upsample – 512 × 64 × 64\n",
        "# Conv 3 × 3 LReLU 256 × 64 × 64\n",
        "# Conv 3 × 3 LReLU 256 × 64 × 64\n",
        "############ step5 (128 * 128)\n",
        "# Upsample – 256 × 128 × 128\n",
        "# Conv 3 × 3 LReLU 128 × 128 × 128\n",
        "# Conv 3 × 3 LReLU 128 × 128 × 128\n",
        "############ step6 (256 * 256)\n",
        "# Upsample – 128 × 256 × 256\n",
        "# Conv 3 × 3 LReLU 64 × 256 × 256\n",
        "# Conv 3 × 3 LReLU 64 × 256 × 256\n",
        "############ step7 (512 * 512)\n",
        "# Upsample – 64 × 512 × 512\n",
        "# Conv 3 × 3 LReLU 32 × 512 × 512\n",
        "# Conv 3 × 3 LReLU 32 × 512 × 512\n",
        "############ step8 (1024 * 1024)\n",
        "# Upsample – 32 × 1024 × 1024\n",
        "# Conv 3 × 3 LReLU 16 × 1024 × 1024\n",
        "# Conv 3 × 3 LReLU 16 × 1024 × 1024\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_channel, leaky_scale=0.2):\n",
        "        super().__init__()\n",
        "        self.initial_layer = nn.Sequential(\n",
        "            EqualizedConvTranspose2d(latent_channel, latent_channel, 4),\n",
        "            nn.LeakyReLU(leaky_scale),\n",
        "            PixelWiseNormalization(),\n",
        "            EqualizedConv2d(latent_channel, latent_channel, 3, padding=1),\n",
        "            nn.LeakyReLU(leaky_scale),\n",
        "            PixelWiseNormalization(),\n",
        "        )\n",
        "        self.initial_toRGB = EqualizedConv2d(latent_channel, 3, 1)\n",
        "\n",
        "\n",
        "        self.block_ch = [512, 512, 512, 512, 256, 128, 64, 32, 16]\n",
        "        self.progressive_blocks = nn.ModuleList()\n",
        "        self.toRGB = nn.ModuleList()\n",
        "        self.toRGB.append(self.initial_toRGB)\n",
        "\n",
        "        for i in range(len(self.block_ch) - 1):\n",
        "            self.progressive_blocks.append(BlockConv2d(self.block_ch[i], self.block_ch[i+1]))\n",
        "            self.toRGB.append(EqualizedConv2d(self.block_ch[i+1], 3, 1))\n",
        "\n",
        "    def forward(self, x, step, alpha):\n",
        "        x = self.initial_layer(x)\n",
        "\n",
        "        if step == 0:\n",
        "            return self.initial_toRGB(x)\n",
        "\n",
        "        for i in range(step):\n",
        "            upsample_out = F.interpolate(x, scale_factor=2)\n",
        "            x = self.progressive_blocks[i](upsample_out)\n",
        "\n",
        "        output_old_layers = (1 - alpha) * self.toRGB[step - 1](upsample_out)\n",
        "        output_new_layer = alpha * self.toRGB[step](x)\n",
        "        return torch.tanh(output_old_layers + output_new_layer)\n",
        "\n",
        "generator = Generator(512)\n",
        "x = torch.randn(2, 512, 1, 1)\n",
        "generator(x, 8, 0.2).size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuqt8cWJoJx1",
        "outputId": "20a32f92-a2a5-4b6c-babf-ca238437ab7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 1024, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}