{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >=(3,8), \"This project requires Python 3.8+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import torch\n",
    "\n",
    "\n",
    "assert version.parse(torch.__version__) >= version.parse(\"2.1.2\"), \"This project requires pytorch 2.1.1 or above!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATASETS & DATALOADERS**\n",
    "\n",
    "* Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "* PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass torch.utils.data.Dataset and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: Image Datasets, Text Datasets, and Audio Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=ToTensor())\n",
    "test_ds = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_ds, val_ds = random_split(train_dataset, [50000, 10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train+val size: \", train_dataset.data.shape)\n",
    "print(\"test size: \", test_ds.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "input_size = [*train_dataset.data.shape[1:]]\n",
    "number_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dloader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in tqdm(range(1, cols * rows + 1)):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, in_feature: int, output_feature: int,   *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_feature, 300)\n",
    "        self.bn1 = nn.BatchNorm1d(300)\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, output_feature)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        output = self.bn1(self.fc1(x))\n",
    "        output = nn.GELU()(output)\n",
    "        output = self.bn2(self.fc2(output))\n",
    "        output = nn.GELU()(output)\n",
    "        return self.fc3(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "saving_path = 'saved_models/fashion_mnist'\n",
    "\n",
    "\n",
    "# model = nn.Sequential(nn.Flatten(),\n",
    "#     nn.Linear(784, 300),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.2),\n",
    "#     nn.Linear(300, 100),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.2),\n",
    "#     nn.Linear(100, 10))\n",
    "\n",
    "model = NN(784, 10)\n",
    "\n",
    "# model description\n",
    "print(\"-\" * 20)\n",
    "print(\"model: \", model)\n",
    "model_total_params = sum(p.numel() for p in model.parameters())\n",
    "model_total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"model total parameters: \", model_total_params)\n",
    "print(\"model total trainable parameters: \", model_total_trainable_params)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f'runs/fashion_trainer_{timestamp}')\n",
    "\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "\n",
    "# Train & validate Network\n",
    "for epoch in range(number_epochs):\n",
    "    print(f'EPOCH {epoch + 1}:')\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    model.train(True)\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_dloader)):\n",
    "\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Make predictions for this batch (forward)\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 500 == 499:\n",
    "            last_loss = running_loss / 500 # loss per batch\n",
    "            print(f'batch {batch_idx + 1} loss: {last_loss}')\n",
    "            tb_x = epoch_number * len(train_dloader) + batch_idx + 1\n",
    "            writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, (vinputs, vlabels) in enumerate(val_dloader):\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {last_loss} valid {avg_vloss}')\n",
    "    \n",
    "    train_losses.append(last_loss)\n",
    "    val_losses.append(avg_vloss)\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : last_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        if os.path.exists(saving_path):\n",
    "            shutil.rmtree(saving_path)\n",
    "        os.makedirs(saving_path)\n",
    "        model_path = f'{saving_path}/model_{timestamp}_{epoch_number}.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = \"Training loss\")\n",
    "plt.plot(val_losses, label = \"Validation loss\")\n",
    "plt.legend(frameon = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "if len(glob.glob(f\"{saving_path}/*.pth\")) > 0:\n",
    "    saved_model_params = glob.glob(f\"{saving_path}/*.pth\")[0]\n",
    "\n",
    "loaded_model = NN(784, 10)\n",
    "# loaded_model = nn.Sequential(nn.Flatten(),\n",
    "#     nn.Linear(784, 300),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(300, 100),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(100, 10))\n",
    "loaded_model.load_state_dict(torch.load(saved_model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootcamp_libs.metrics.accuracy import check_accuracy\n",
    "\n",
    "print(\"train accuracy: \", check_accuracy(train_dloader, loaded_model).item())\n",
    "print(\"val accuracy: \", check_accuracy(val_dloader, loaded_model).item())\n",
    "print(\"test accuracy: \", check_accuracy(test_dloader, loaded_model).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
