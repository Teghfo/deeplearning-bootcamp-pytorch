{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "on9qhN8MHjam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eEz79GQHQLB",
        "outputId": "b2d5c05e-a8d4-4fce-83a5-fe6e63f9892a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0   9.9M      0  0:00:08  0:00:08 --:--:-- 16.3M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"aclImdb\""
      ],
      "metadata": {
        "id": "E7iK339JHtUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2740k68NH2BX",
        "outputId": "a4fe0e56-d05d-42a0-925d-29cfc2bc8646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(\"aclImdb/train/neg/\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlR42YRNuGSX",
        "outputId": "37cd9e43-3e28-4264-900d-ffc586b558d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12500"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path_):\n",
        "  dataset = []\n",
        "\n",
        "  for data_path in os.listdir(path_):\n",
        "    with open(os.path.join(path_, data_path)) as f:\n",
        "      dataset.append(f.read())\n",
        "  return dataset\n",
        "\n",
        "pos_ = create_dataset(os.path.join(DATA_DIR, \"train\", \"pos\"))\n",
        "neg_ = create_dataset(os.path.join(DATA_DIR, \"train\", \"neg\"))\n",
        "dataset = pos_ + neg_"
      ],
      "metadata": {
        "id": "5g70uuc5iBBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN1ijCXainZ3",
        "outputId": "6eeb7acb-b0f2-43a2-99b6-ac5f291e5bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Vectorizer:\n",
        "\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text\n",
        "                  if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in tqdm(dataset):\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "      self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
      ],
      "metadata": {
        "id": "bBWIdl1bgcvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "\n",
        "  def __init__(self, freq=1):\n",
        "    self.stoi = {\"<pad>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    self.itos = {0: \"<pad>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "\n",
        "    self.freq = freq\n",
        "\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text\n",
        "                  if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    temp_vocab = {}\n",
        "\n",
        "    for text in tqdm(dataset):\n",
        "\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "\n",
        "      for token in tokens:\n",
        "        if token not in temp_vocab:\n",
        "          temp_vocab[token] = 1\n",
        "        else:\n",
        "          temp_vocab[token] +=1\n",
        "\n",
        "        if temp_vocab[token] == self.freq:\n",
        "          indx = len(self.stoi)\n",
        "          self.stoi[token] = indx\n",
        "          self.itos[indx] = token\n",
        "\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return ([self.stoi[\"<SOS>\"]] + [self.stoi.get(token, 3) for token in tokens]\n",
        "            + [self.stoi[\"<EOS>\"]])\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.itos.get(i, \"<UNK>\") for i in int_sequence)"
      ],
      "metadata": {
        "id": "JtDhGWiwv2jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Vocabulary()\n",
        "tokenizer.make_vocabulary(dataset)"
      ],
      "metadata": {
        "id": "Z_bHGjKejbbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_index = tokenizer.encode(dataset[10])\n",
        "tokenizer.decode(text_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "uP_HjAid6c1-",
        "outputId": "0448a2a1-49fe-41cb-aa6e-132b442072d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<SOS> a strong woman oriented subject after long director krishna vamsis shakti the power the desi version of the hollywood hit not without my daughter is actress sridevis first homeproduction a story about a womans fight against harsh injusticebr br the story of the film revolves around nandini karisma kapoor who lives in canada with her two uncles tiku talsania jaspal bhatti there she meets shekhar sanjay kapoor falls in love with him and they soon marry their family is complete when nandini has a boy raja master jai gidwani but their happiness is short lived as the news of shekhars ailing mother deepti navalmakes them leave their perfect life in canada and come to india and thats when the problems start from the moment they reachbr br india both are shocked to see the pollution and the vast throngs of people everywhere they take a crowded train to reach shekhars village and when they finally reach the station they have to catch a long bus drive to his village the filthy sweaty bus combined with the uncertain terrain makes it a neverending drive and unfortunately for them a frenzied mob that beat shekhar out of shape for no fault of his attacks their bus fortunately they get shot dead just in time before they can further harm him after that they drive to the handing havel where shekhars father narsimha nana patekar lives with his wife deepti naval nandani realized that her fatherinlaw is in command as soon as she enters the place but her only solace is her motherinlaws warm welcomebr br living there nandini learns of her fatherinlaws tyrannical behavior and realizes that ruthless killing is a way of life for him the day she sees her fatherinlaw teach her son to throw a bomb she loses it and lashes out against him insisting to shekhar that they move back to canada but terror strikes again when shekhar is murdered one day leaving a broken down nandini alone with her son in this strange land where she is harrowed by a cruel fatherinlaw her fight against this man to save her son is what makes up the climax of this emotional heartwrenching filmbr br what sets apart shakti from most films being made off late is also the rural setting of the movie the only drawback is ismail darbars music which fails to rise above the script the only saving grace is the sexy item number ishq kameena which has been composed by anu malik another pat for the director comes because he has extracted some splendid performances from his cast karisma kapoor is the life of the film and has given a moving performance as a helpless mother she is sure to win awards for this heated portrayal second is actor nana patekar who is back with a bang with this film his uncouth mannerisms suit him to the hilt and hes shown his versatility once again with this role sanjay kapoor is the surprise packet of the film with a sincere and effective portrayal that stands up against both the other actors deepti naval too is in top form and her prclimax showdown with nana is praiseworthy shahrukhs cameo provides the lighter moments and surely hes been pulled in to get the required star value though his role was not really required hes done it well overall shakti is a far superior film than most churned out these days and the prrelease hype is sure to get it a good opening shakti is sure to get the critics and audience thumps up so what if the film needs to be desperately trimmed by at least 2 reels to better the impact shakti still has the power to go on without a hitch <EOS>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "def build_vocab(data_dir: str, tokenizer: Vocabulary) -> Vocabulary:\n",
        "  dataset = []\n",
        "  path_list = []\n",
        "\n",
        "  path_ = os.path.join(data_dir, \"train\")\n",
        "  path_list = (glob(os.path.join(path_, \"pos\", \"*.txt\")) +\n",
        "                    glob(os.path.join(path_, \"neg\", \"*.txt\")))\n",
        "\n",
        "  for data_path in path_list:\n",
        "    with open(data_path) as f:\n",
        "      dataset.append(f.read())\n",
        "\n",
        "\n",
        "  tokenizer.make_vocabulary(dataset)\n",
        "  return tokenizer\n",
        "\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_dir, tokenizer: Vocabulary,\n",
        "               train=True, transform=None, target_transform=None,\n",
        "               random_state=42):\n",
        "\n",
        "    self.data_dir = data_dir\n",
        "    self.data = []\n",
        "\n",
        "    if train:\n",
        "      path_ = os.path.join(data_dir, \"train\")\n",
        "    else:\n",
        "      path_ = os.path.join(data_dir, \"test\")\n",
        "\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        data_path = os.path.join(path_, label)\n",
        "        for i in glob(data_path + \"/*.txt\"):\n",
        "          self.data.append((i, label == \"pos\"))\n",
        "\n",
        "    random.Random(random_state).shuffle(self.data)\n",
        "\n",
        "    self.nlp = tokenizer\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, indx):\n",
        "    path_data, label = self.data[indx]\n",
        "\n",
        "    with open(path_data, \"r\") as f:\n",
        "      text_data = f.read()\n",
        "      data = self.nlp.encode(text_data)\n",
        "    return torch.tensor(data).long(), torch.tensor([label])\n"
      ],
      "metadata": {
        "id": "kcDPqhyMLVLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Vocabulary(freq=2)\n",
        "nlp = build_vocab(DATA_DIR, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKos5CEF-g7y",
        "outputId": "dad5c3f0-6f30-4481-e8eb-207fcdeab2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25000/25000 [00:13<00:00, 1893.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(nlp.stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CrcTfRmAASk",
        "outputId": "9be17632-9519-4f34-aa39-d681ceb32907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57553"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = IMDBDataset(DATA_DIR, tokenizer=nlp, train=True)\n",
        "print(\"train dataset size:\", len(train_dataset))\n",
        "test_dataset = IMDBDataset(DATA_DIR, tokenizer=nlp, train=False)\n",
        "print(\"test dataset size:\", len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuPKv6nndhOu",
        "outputId": "d40b564e-9bf3-40c8-e9b7-d9b210075881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset size: 25000\n",
            "test dataset size: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def pad_collate(x):\n",
        "  data = [item[0].unsqueeze(1) for item in x]\n",
        "  label = [item[1] for item in x]\n",
        "  padded_data = pad_sequence(data)\n",
        "  return padded_data.squeeze(), torch.tensor(label)"
      ],
      "metadata": {
        "id": "RXZnA5ngJq5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: better padding!"
      ],
      "metadata": {
        "id": "U_QF5sQlAebN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
        "test_dl = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "O-5EuKYsByHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item, label in train_dl:\n",
        "  print(f\"data size {item.size()}\")\n",
        "  print(f\"label size {label.size()}\")\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXbkGEelG8ef",
        "outputId": "40f1c784-11d2-4fcd-ec42-3a338d21bc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data size torch.Size([924, 32])\n",
            "label size torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features, _ = next(iter(train_dl))\n",
        "embedding_ = nn.Embedding(len(nlp.stoi), 256, padding_idx=0)\n",
        "embedding_(features).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6lGmKK1DJoc",
        "outputId": "ea8df874-cd7b-4605-8e97-5a282510f126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([486, 32, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, hidden_size=128,\n",
        "                 embedding_size=400, n_layers=2, dropout=0.2):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ =  self.lstm(x)\n",
        "        x = x[-1, : , :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ru8agm7T_NCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13ZRcQfEG2Nv",
        "outputId": "e8c3a609-d278-45e2-f8ea-1056522c6a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model hyperparamters\n",
        "vocab_size = len(nlp.stoi)\n",
        "output_size = 1\n",
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "grad_clip = 4\n",
        "n_layers = 2\n",
        "epochs_num = 10\n",
        "\n",
        "# model initialization\n",
        "model = LSTMModel(vocab_size, output_size, hidden_size, embedding_size,\n",
        "                  n_layers).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ9bF4PjBdLs",
        "outputId": "9f12a10a-4b27-40cc-f7ac-19031c0b3888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMModel(\n",
            "  (embedding): Embedding(57553, 256, padding_idx=0)\n",
            "  (lstm): LSTM(256, 512, num_layers=2, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = next(iter(train_dl))\n",
        "model(data.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4XYm_wkC5BF",
        "outputId": "c8ca6541-8d3e-4968-bf3b-1f9adab5d32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5085],\n",
              "        [0.5084],\n",
              "        [0.5105],\n",
              "        [0.5108],\n",
              "        [0.5085],\n",
              "        [0.5099],\n",
              "        [0.5092],\n",
              "        [0.5061],\n",
              "        [0.5067],\n",
              "        [0.5104],\n",
              "        [0.5076],\n",
              "        [0.5067],\n",
              "        [0.5072],\n",
              "        [0.5058],\n",
              "        [0.5096],\n",
              "        [0.5122],\n",
              "        [0.5076],\n",
              "        [0.5061],\n",
              "        [0.5081],\n",
              "        [0.5107],\n",
              "        [0.5133],\n",
              "        [0.5063],\n",
              "        [0.5089],\n",
              "        [0.5083],\n",
              "        [0.5070],\n",
              "        [0.5096],\n",
              "        [0.5126],\n",
              "        [0.5035],\n",
              "        [0.5056],\n",
              "        [0.5092],\n",
              "        [0.5092],\n",
              "        [0.5085]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "vfRL_UmFGlMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs_num):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0\n",
        "\n",
        "    for id, (data, label) in enumerate(tqdm(train_dl)):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        prediction = model(data)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(prediction.squeeze(), label.float())\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        if id % 200 == 199:\n",
        "          print(f\"\\n Epoch {epoch+1}/{epochs_num}| step {id+1}/{len(train_dl)} train_loss: {train_loss/(id + 1):.4f}\")"
      ],
      "metadata": {
        "id": "uKjvRNliHN34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e51e90a-d5ee-4561-d6d8-544c4bd1d0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/782 [02:11<28:36:27, 131.87s/it]"
          ]
        }
      ]
    }
  ]
}